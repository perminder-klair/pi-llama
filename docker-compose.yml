services:
  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server
    ports:
      - "5000:5000"
    volumes:
      - ~/llama.cpp/models:/models:ro
      - ./client/.output/public:/static:ro
    command: >
      --model /models/${MODEL_NAME:-Qwen3-4B-Q4_K_M.gguf}
      --host 0.0.0.0
      --port 5000
      -c ${CONTEXT_SIZE:-8192}
      --embedding
      --jinja
      --path /static
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  memory-api:
    build: ./memory-api
    ports:
      - "4000:4000"
    volumes:
      - memory-data:/app/data
    environment:
      - LLAMA_SERVER_URL=http://llama-server:5000
      - DATA_DIR=/app/data
    depends_on:
      llama-server:
        condition: service_healthy

volumes:
  memory-data:
