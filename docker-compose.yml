services:
  gateway:
    image: nginx:alpine
    ports:
      - "3080:80"
    volumes:
      - ./configs/nginx-gateway.conf:/etc/nginx/nginx.conf:ro
      - ./client/.output/public:/static:ro
    depends_on:
      - llama-server
      - memory-api
      - whisper
      - tts
      - vosk

  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server
    volumes:
      - ./models:/models:ro
    command: >
      --model /models/${MODEL_NAME:-Qwen3-4B-Q4_K_M.gguf}
      --host 0.0.0.0
      --port 5000
      -c ${CONTEXT_SIZE:-8192}
      --embedding
      --jinja
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  memory-api:
    build: ./memory-api
    volumes:
      - memory-data:/app/data
    environment:
      - LLAMA_SERVER_URL=http://llama-server:5000
      - DATA_DIR=/app/data
    depends_on:
      llama-server:
        condition: service_healthy

  # Speech-to-Text (Whisper) - CPU optimized
  whisper:
    image: onerahmet/openai-whisper-asr-webservice:latest
    volumes:
      - whisper-cache:/root/.cache
    environment:
      - ASR_MODEL=small
      - ASR_ENGINE=faster_whisper

  # Text-to-Speech (Piper) - CPU optimized
  tts:
    image: ghcr.io/matatonic/openedai-speech-min
    volumes:
      - tts-voices:/app/voices
      - tts-config:/app/config

  # Live Speech-to-Text (Vosk) - CPU optimized
  vosk:
    image: alphacep/kaldi-en:latest
    ports:
      - "2700:2700"

volumes:
  memory-data:
  whisper-cache:
  tts-voices:
  tts-config:
