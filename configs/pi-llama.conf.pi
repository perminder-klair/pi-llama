# Pi-LLaMA Configuration - Raspberry Pi Preset
# Copy this file to pi-llama.conf in the project root

# Environment: pi | desktop
ENVIRONMENT="pi"

# Package manager: apt | pacman | dnf
PACKAGE_MANAGER="apt"

# Server configuration
SERVER_PORT=5000
SERVER_HOST="0.0.0.0"

# Model configuration
MODEL_NAME="qwen2.5-0.5b-instruct-q4_0.gguf"
MODEL_URL="https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q4_0.gguf"

# llama-server parameters
CONTEXT_SIZE=2048
GPU_LAYERS=0
CPU_THREADS=2
ENABLE_TOOL_CALLING=false
ENABLE_EMBEDDING=false

# Service configuration (Pi only)
USE_SYSTEMD=true
USE_NGINX=true
NGINX_PORT=80

# Paths (leave empty for auto-detection)
LLAMA_DIR=""
MODEL_DIR=""
