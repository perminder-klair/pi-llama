# Pi-LLaMA Configuration - Desktop Preset
# Copy this file to pi-llama.conf in the project root

# Environment: pi | desktop
ENVIRONMENT="desktop"

# Package manager: apt | pacman | dnf
PACKAGE_MANAGER="pacman"

# Server configuration
SERVER_PORT=5000
SERVER_HOST="0.0.0.0"

# Model configuration
MODEL_NAME="Qwen3-30B-A3B-Q4_K_M.gguf"
MODEL_URL="https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_K_M.gguf"

# llama-server parameters
CONTEXT_SIZE=8192
GPU_LAYERS=25
CPU_THREADS=8
ENABLE_TOOL_CALLING=true
ENABLE_EMBEDDING=true

# Service configuration (not used for desktop)
USE_SYSTEMD=false
USE_NGINX=false
NGINX_PORT=80

# Paths (leave empty for auto-detection)
LLAMA_DIR=""
MODEL_DIR=""
