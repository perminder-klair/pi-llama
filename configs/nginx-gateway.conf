events { worker_connections 1024; }

http {
    include /etc/nginx/mime.types;

    # SSE/streaming support
    proxy_buffering off;
    proxy_cache off;

    upstream llama { server llama-server:5000; }
    upstream memory { server memory-api:4000; }
    upstream whisper { server whisper:9000; }
    upstream tts { server tts:8000; }

    server {
        listen 80;
        root /static;
        index index.html;

        # Use relative redirects to preserve the original port
        absolute_redirect off;

        # LLaMA API (OpenAI-compatible)
        location /v1/ {
            proxy_pass http://llama;
            proxy_http_version 1.1;
            proxy_set_header Connection "";
        }

        # Memory API
        location /memory/ {
            rewrite ^/memory/(.*) /$1 break;
            proxy_pass http://memory;
        }

        # Whisper (Speech-to-Text)
        location /whisper/ {
            rewrite ^/whisper/(.*) /$1 break;
            proxy_pass http://whisper;
        }

        # TTS (Text-to-Speech)
        location /tts/ {
            rewrite ^/tts/(.*) /$1 break;
            proxy_pass http://tts;
        }

        # LLM health check
        location = /health {
            proxy_pass http://llama;
            proxy_http_version 1.1;
            proxy_set_header Connection "";
        }

        # Static files (React app) - SPA fallback
        location / {
            try_files $uri $uri/ /index.html;
        }
    }
}
